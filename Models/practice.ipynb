{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question 1: Closed Source Chat Model Control üå°Ô∏è\n",
    "\n",
    "Using the **Anthropic Claude** chat model interface:\n",
    "1.  Initialize the model using `ChatAnthropic`.\n",
    "2.  Set the `temperature` parameter to `0.9` (high creativity) and `max_tokens` to `50`.\n",
    "3.  Send the following prompt: \"Write a short, dramatic movie tagline for a film about a rogue AI that falls in love with a human.\"\n",
    "4.  Print the `content` of the result.\n",
    "\n",
    "*(Focus: Applying key control parameters (temperature, max tokens) to a closed-source chat interface, and ensuring you extract the `.content` correctly.)*"
   ],
   "id": "579d39b3d2eebe88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:21:40.638056Z",
     "start_time": "2025-11-15T14:21:39.402187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv"
   ],
   "id": "365fdbd47707e13",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:21:45.319123Z",
     "start_time": "2025-11-15T14:21:43.576671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "model = ChatAnthropic(model_name=\"claude-3-7-sonnet-20250219\", temperature=0.9, max_tokens=50)\n",
    "result = model.invoke(\"Write a short, dramatic movie tagline for a film about a rogue AI that falls in love with a human.\")\n",
    "print(result.content)"
   ],
   "id": "bdd0911877d85149",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"In a world where code meets compassion, their forbidden connection could save humanity... or delete it forever.\"\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question 2: Open Source Local Inference üíæ\n",
    "\n",
    "Implement the code to run a Hugging Face model **locally** on your machine using the `HuggingFacePipeline` class.\n",
    "1.  Import the necessary classes (`ChatHuggingFace` and `HuggingFacePipeline`).\n",
    "2.  Use the model ID `google/gemma-2b` (or another suitable small model).\n",
    "3.  Set the `pipeline_kwargs` to ensure a maximum generation of **75 new tokens**.\n",
    "4.  Ask the model: \"Explain the main difference between an LLM and a Chat Model in three sentences.\"\n",
    "5.  Run the code and print the output.\n",
    "\n",
    "*(Focus: Implementing the local pipeline architecture, understanding the setup for local models, and handling keyword arguments.)*"
   ],
   "id": "ebc8cf8c48301d40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:23:01.383695Z",
     "start_time": "2025-11-15T14:21:49.603020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/gemma-2b-it\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=75\n",
    "    )\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "response = model.invoke(\"Explain the main difference between an LLM and a Chat Model in three sentences.\")\n",
    "print(response.content)"
   ],
   "id": "20593cd7879f4fec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7882e41d665c42cf878d8ace86157284"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "711502fac5b24da0a639ada66c574e2e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivansh Gupta\\OneDrive\\Desktop\\Data Science\\My Data Science\\Langchain-Tutorials\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shivansh Gupta\\.cache\\huggingface\\hub\\models--google--gemma-2b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Explain the main difference between an LLM and a Chat Model in three sentences.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's the difference between an LLM and a Chat Model:\n",
      "\n",
      "1. **LLM (Large Language Model)** is a specialized AI model with a massive dataset of text and code, trained on a massive dataset of text and code.\n",
      "\n",
      "2. **Chat model** is a type of AI model designed to engage in human-like conversation.\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question 3: Vector Generation and Verification üìè\n",
    "\n",
    "Use the **OpenAI Embedding Model** (`OpenAIEmbeddings`) to generate vectors for a list of three sentences:\n",
    "1.  \"The sun rises in the east.\"\n",
    "2.  \"A computer processes data.\"\n",
    "3.  \"The capital of France is Paris.\"\n",
    "4.  Use a target dimension of `64` for the output vectors.\n",
    "5.  Call the correct function to process all three texts simultaneously.\n",
    "6.  **Print:** The number of vectors generated, and the dimension (length) of the first vector, verifying that it is 64.\n",
    "\n",
    "*(Focus: Differentiating between `embed_query` and `embed_documents`, and controlling/verifying the output dimensions.)*"
   ],
   "id": "d1daecb1d9b1e44f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T16:14:37.371410Z",
     "start_time": "2025-11-15T16:14:32.922838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=64)\n",
    "\n",
    "docs = [\n",
    "    \"The sun rises in the east.\",\n",
    "    \"A computer processes data.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "]\n",
    "\n",
    "result = model.embed_documents(docs)\n",
    "print(result)"
   ],
   "id": "d958d60a86248b48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07756446301937103, -0.14171405136585236, -0.0034877683501690626, -0.07713549584150314, -0.14522375166416168, -0.19264376163482666, 0.03531152382493019, 0.15005934238433838, 0.014818750321865082, -0.20278289914131165, -0.0793193131685257, 0.21619777381420135, 0.09172026813030243, -0.05202161520719528, 0.11355842649936676, 0.1859363168478012, 0.0099002905189991, 0.09819371998310089, -0.058495067059993744, 0.1449897736310959, -0.15629881620407104, 0.0792413204908371, -0.14795352518558502, 0.13227684795856476, -0.12385355681180954, 0.1532570719718933, -0.004323760513216257, -0.0326792448759079, 0.05646723881363869, -0.0649295225739479, 0.1909279078245163, 0.09936362504959106, 0.27188506722450256, 0.08080118894577026, -0.21572981774806976, 0.16986967623233795, 0.008613399229943752, 0.05857305973768234, -0.11714611947536469, 0.16409815847873688, -0.13617651164531708, -0.1307949721813202, -0.0933581292629242, 0.017412031069397926, -0.269857257604599, -0.059820957481861115, 0.01959584839642048, 0.13290078938007355, 0.02378799393773079, -0.029033051803708076, -0.11332444846630096, -0.20262691378593445, 0.05966496840119362, -0.04410528019070625, -0.17579717934131622, 0.14561372995376587, -0.121279776096344, 0.0745227187871933, 0.01070459745824337, -0.007599484175443649, 0.17454928159713745, -0.08509082347154617, -0.13383671641349792, 0.0836089476943016], [-0.048254676163196564, 0.045786406844854355, -0.061953574419021606, 0.1179010197520256, 0.2032208889722824, -0.02822055108845234, 0.025813989341259003, 0.29273679852485657, 0.013554914854466915, 0.18808215856552124, 0.1667727679014206, -0.28483834862709045, -0.14933031797409058, 0.022234996780753136, 0.032231491059064865, 0.028076568618416786, -0.17590536177158356, 0.024806112051010132, 0.016928216442465782, -0.22675171494483948, 0.01703106239438057, -0.01776125840842724, -0.1241539716720581, 0.11263538151979446, -0.25307992100715637, 0.08507303148508072, -0.05389055982232094, -0.0499824658036232, -0.06244722753763199, 0.03245774656534195, 0.10259775072336197, 0.22987820208072662, -0.05245073512196541, -0.20980294048786163, 0.04796671122312546, -0.04327699914574623, -0.07079820334911346, -0.04097327962517738, 0.2032208889722824, -0.027665190398693085, 0.06993430852890015, -0.08157631754875183, -0.05915620177984238, 0.04175489768385887, 0.05195707827806473, -0.1279386579990387, 0.04023279994726181, -0.1806773543357849, -0.011210058815777302, -0.027397794649004936, -0.03751770034432411, -0.21737229824066162, 0.2525862753391266, 0.09988265484571457, -0.2170431911945343, 0.0891045406460762, 0.07046910375356674, 0.10259775072336197, -0.12892596423625946, -0.1317233294248581, 0.0603080615401268, 0.11140124499797821, -0.09206646680831909, 0.03332164138555527], [-0.11303596943616867, 0.2871444821357727, -0.02134951390326023, 0.229177325963974, -0.19481107592582703, -0.11935025453567505, 0.06562711298465729, -0.015384589321911335, -0.18880733847618103, -0.22503681480884552, 0.027430890128016472, 0.18570195138454437, -0.046296097338199615, -0.09238516539335251, 0.05713906139135361, 0.02318686619400978, 0.0013796314597129822, 0.09533528238534927, -0.13104718923568726, 0.1287699192762375, 0.07810040563344955, -0.06562711298465729, -0.2722386419773102, -0.10661818087100983, -0.08213739842176437, 0.175040140748024, 0.09647392481565475, -0.05413719266653061, 0.12100645899772644, 0.14108793437480927, 0.3945907652378082, -0.013159063644707203, 0.1094130203127861, -0.1720382571220398, -0.12162753194570541, -0.0990617424249649, 0.058122433722019196, 0.008636849001049995, 0.06495428085327148, 0.05315382033586502, 0.1028917133808136, 0.03604833036661148, -0.04220734164118767, 0.11469217389822006, 0.10356455296278, -0.04769352078437805, 0.020301446318626404, -0.04531272500753403, 0.07846269756555557, 0.0895385667681694, -0.13642986118793488, 0.03317585214972496, -0.06148659810423851, 0.08591561764478683, -0.14584952592849731, -0.051238831132650375, 0.07245895266532898, -0.14460738003253937, -0.1565113365650177, -0.06650696694850922, -0.08922802656888962, -0.03956776484847069, -0.1605483442544937, 0.0444328673183918]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T16:14:57.126992Z",
     "start_time": "2025-11-15T16:14:57.121007Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(result[0]))",
   "id": "55ea281b9e78914b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question 4: Cosine Similarity Calculation üìê\n",
    "\n",
    "You have two search terms:\n",
    "*   **Query A:** \"A device used for quick mathematical calculations.\"\n",
    "*   **Query B:** \"The large mammal with a trunk.\"\n",
    "\n",
    "Implement a similarity search that finds the semantic similarity score (using Cosine Similarity) between **Query A** and the following **Document:** \"A calculator is a portable electronic device used to perform arithmetic operations.\"\n",
    "\n",
    "1.  Generate embeddings for the Query A and the Document text.\n",
    "2.  Calculate and print the Cosine Similarity score between them.\n",
    "3.  **Bonus:** Calculate and print the Cosine Similarity score between Query B and the Document text. (The score for B should be significantly lower than A, demonstrating semantic context.)\n",
    "\n",
    "*(Focus: Using the `cosine_similarity` function from `sklearn` and properly formatting the inputs (2D list required) for vector comparison.)*"
   ],
   "id": "8dcd07f23b827063"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T16:21:56.441657Z",
     "start_time": "2025-11-15T16:21:52.908783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=64)\n",
    "\n",
    "query_a = \"A device used for quick mathematical calculations.\"\n",
    "query_b = \"The large mammal with a trunk.\"\n",
    "\n",
    "docs = [\"A calculator is a portable electronic device used to perform arithmetic operations.\"]\n",
    "\n",
    "query_a_embedding = model.embed_query(query_a)\n",
    "query_b_embedding = model.embed_query(query_b)\n",
    "docs_embedding = model.embed_documents(docs)\n",
    "\n",
    "query_a_cosine_similarity = cosine_similarity([query_a_embedding], docs_embedding)[0]\n",
    "print(query_a_cosine_similarity)\n",
    "query_b_cosine_similarity = cosine_similarity([query_b_embedding], docs_embedding)[0]\n",
    "print(query_b_cosine_similarity)"
   ],
   "id": "f9e750df4c8373bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63773452]\n",
      "[0.16919289]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Question 5: Implement Custom LLM, ChatModel and Embedding Model.",
   "id": "c279ec2decdb7854"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24c10cca6d833c07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
