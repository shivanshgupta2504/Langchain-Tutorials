## Detailed Summary Notes for Revision

### I. LLM Parameter Correction: Temperature

The video begins with a rectification regarding the **Temperature** parameter used when interacting with Large Language Models (LLMs):

*   **Range:** Temperature values typically lie between 0 and 2.
*   **Deterministic Output (Temperature ≈ 0):** Setting the temperature to 0 (or close to 0) ensures that for the **same input**, the LLM produces the **exactly same output** every time. This is useful for applications requiring consistency.
*   **Creative Output (Temperature ≈ 1.5–2):** Increasing the temperature results in a more **creative** and varied output. The same input will yield a slightly different, or significantly different, response each time. This is useful when diverse responses are desired.

### II. Understanding Prompts

*   **Definition:** A Prompt is simply the message you send to the LLM.
*   **Types:** While **Multimodal Prompts** (using images, sound, or video) exist, the video focuses entirely on **Text-Based Prompts**, which are used 99% of the time currently.
*   **Importance:** Prompts are **super important** because the LLM’s output is highly sensitive to them; even minor changes can significantly alter the result. The high importance of prompts has led to the emergence of the job profile **Prompt Engineering**.

### III. Static vs. Dynamic Prompts

#### A. Static Prompts

*   **Definition:** The user or programmer inputs the **entire prompt** directly into the model call (e.g., `"Summarize X paper in five lines"`).
*   **Problem:** This approach gives the user too much control, leading to high chances of receiving an **inconsistent** or undesirable output. If the user makes a mistake (e.g., misspelling a research paper title), the LLM might **hallucinate** or provide an incorrect summary. Static prompts make it difficult to guarantee a consistent user experience across all users (e.g., ensuring the summary always includes analogies or mathematical details).

#### B. Dynamic Prompts

*   **Definition:** A fixed **template** is prepared with specific **placeholders** (fill-in-the-blanks). User input is only used to fill these predefined variables.
*   **Solution:** LangChain uses the **`PromptTemplate`** class to achieve this. The template contains specific instructions (like "Include relevant mathematical equations," "Use simple intuitive code snippets," or "Add analogies") that ensure the quality and consistency of the output, regardless of the user's input variables (like paper name, length, or style).

### IV. Advantages of `PromptTemplate` over F-strings

Although Python F-strings could technically create dynamic strings, the `PromptTemplate` class is preferred in LangChain due to three key reasons:

1.  **Validation:** It offers built-in validation. You can set `validate_template=True`.
    *   It checks that **all required input variables** (placeholders) in the template are provided when invoked.
    *   It checks that **no extra variables** are provided that are not defined in the template.
2.  **Reusability:** Templates can be easily saved externally as a **JSON file** using the `.save()` function, and then loaded using `load_prompt()` for reuse across different parts of a large application.
3.  **Tight Coupling with the LangChain Ecosystem:** `PromptTemplate` is designed to fit seamlessly with other components, specifically **Chains**. This allows the process of template creation, filling, and model invocation to be chained and called in a single step.

### V. Managing Conversation Context and History

*   **The Problem of Context:** A basic LLM call lacks memory; it treats every new query as an independent, stand-alone request. It cannot remember previous turns in a conversation, making multi-turn dialogue impossible (e.g., forgetting which number was previously identified as "the bigger number"). That is why we say that LLMs are **stateless**
*   **Initial Solution:** Maintain a **list of messages (Chat History)** and send the entire list to the LLM with every new invocation.
*   **The Labelling Problem:** Simply storing messages as strings is insufficient because the LLM cannot tell **who** sent which message (User vs. AI), which is crucial for maintaining context.

#### A. LangChain Message Types

LangChain addresses the labelling issue by providing three specific message classes to categorize conversation turns:

1.  **`SystemMessage`:** Defines the LLM's role, persona, or operating instructions (e.g., "You are a helpful assistant"). This is typically added at the start of the conversation history.
2.  **`HumanMessage`:** Messages sent by the user to the LLM.
3.  **`AIMessage`:** Messages generated by the LLM in response.
    *   By converting user input to `HumanMessage` and LLM output to `AIMessage`, every part of the history is clearly labelled, ensuring the LLM understands the conversational flow.

### VI. Dynamic Conversation Templates

*   **Invocation Modes:** The LLM's `.invoke()` function supports two modes: taking a single message (for stand-alone queries) or taking a list of messages (for multi-turn conversations).
*   **`ChatPromptTemplate`:** This class is used when working with a **list of messages** (multi-turn conversations) that require dynamic content (placeholders).
    *   Example: Creating a dynamic system message where the expertise domain is filled at runtime (`"You are a helpful {domain} expert"`).
    *   *Note on Usage:* When defining messages inside `ChatPromptTemplate`, the recommended syntax is to pass tuples `(role, content_string)` rather than using the `SystemMessage` or `HumanMessage` classes directly.

### VII. Managing Existing History: `MessagePlaceHolder`

*   **Definition:** A special placeholder used *inside* a `ChatPromptTemplate` to dynamically insert an entire **list of messages (chat history)** at runtime.
*   **Use Case:** When building professional chatbots (like customer support tools), historical conversation data must often be loaded from an external database. `MessagesPlaceholder` allows you to define a spot in the prompt template—typically between the system message and the current human query—where this previous history will be inserted, ensuring the LLM has full context for the current question.

***

## Code Examples Provided in the Video

The following are key conceptual code examples demonstrated in the transcript:

### 1. `PromptTemplate` Creation (Dynamic Prompting)

This code defines a structured template with placeholders and specifies the expected variables:

```python
from langchain_core.prompts import PromptTemplate

# Define the detailed template structure
template_string = """Please summarize the research paper titled {paper_input} with the following specifications: 
Explanation Style: {style_input} 
Explanation Length: {length_input} 
Include relevant mathematical equations if present in the paper. Explain the mathematical concept using simple intuitive code snippets where applicable. Also add analogies that are relatable. If any piece of information is not available, instead of hallucinating simply put: Insufficient Information Available. 
Ensure the summary is clear, accurate and align with the provided style and length."""

# Create the template object, specifying input variables
prompt_template = PromptTemplate(
    template=template_string,
    input_variables=["paper_input", "style_input", "length_input"]
)

# Example: Invoking the template with values
# Assuming variables paper_input, style_input, length_input are set by user UI
prompt = prompt_template.invoke({
    "paper_input": paper_input, 
    "style_input": style_input, 
    "length_input": length_input
})
```

### 2. Basic Chatbot Structure (Without Message Classes)

This demonstrates the core loop logic before implementing context/history awareness:

```python
# Assuming 'model' (ChatOpenAI) is defined earlier
while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break
    
    # Static prompt invocation
    result = model.invoke(user_input)
    print(f"AI: {result.content}")
```

### 3. Context-Aware Chatbot (Using Message Classes)

This example utilizes LangChain message types to label the speaker and maintain accurate chat history (context):

```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# Initialize Chat History with a System Message
chat_history = [
    SystemMessage(content="You are a helpful AI Assistant")
]

# Assuming 'model' is defined
while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break

    # 1. Convert user input to a Human Message and append to history
    chat_history.append(HumanMessage(content=user_input))
    
    # 2. Invoke model with the entire history (list of messages)
    result = model.invoke(chat_history)
    
    # 3. Convert LLM output to an AI Message and append to history
    chat_history.append(AIMessage(content=result.content))
    
    print(f"AI: {result.content}")
```

### 4. `ChatPromptTemplate` (Dynamic Multi-Turn Prompts)

This illustrates how to create a template for a list of messages where parts of the role or query are dynamic, using the tuple syntax:

```python
from langchain_core.prompts import ChatPromptTemplate

# Create a Chat Template with dynamic placeholders ({domain} and {topic})
chat_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful {domain} expert"),
    ("human", "Explain in simple terms what is {topic}"),
])

# Invoke the template to create the list of messages
prompt = chat_template.invoke({
    "domain": "Cricket",
    "topic": "The Duckworth-Lewis method"
})
# 'prompt' is now a list containing a dynamic SystemMessage and a dynamic HumanMessage
```

### 5. `MessagesPlaceholder` (Inserting External History)

This shows how to integrate existing chat history (e.g., loaded from a database) into a new prompt template using a placeholder:

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# 1. Define the pre-existing chat history (loaded from storage)
previous_history = [
    HumanMessage(content="I want to request a refund for order 12345."),
    AIMessage(content="Your refund request has been initiated. You will get your refund in 3-5 business days.")
]

# 2. Create the Chat Template structure
chat_template = ChatPromptTemplate.from_messages([
    SystemMessage(content="You are a helpful customer support agent"), # Fixed role
    MessagesPlaceholder(variable_name="chat_history"),                 # Placeholder for dynamic history insertion
    ("human", "{query}")                                              # Placeholder for the current query
])

# 3. Invoke the template, passing the loaded history into the placeholder variable
final_prompt = chat_template.invoke({
    "chat_history": previous_history, # List of messages goes here
    "query": "Where is my refund?"    # Current human query
})
```

***

## Practice Questions

Use the provided concepts and coding methods to practice implementing prompt techniques in LangChain.

### Conceptual Revision Questions

1.  **Temperature & Consistency:** You are building a tool that generates concise, factual definitions for medical terms. Should you set the LLM's temperature parameter high (e.g., 1.5) or low (e.g., 0)? Explain your choice based on the need for determinism.
2.  **Static Prompt Risk:** Describe a scenario in a research tool where relying on a static prompt (where the user provides all the instructions) could lead to an undesirable output or LLM hallucination. How does using a dynamic prompt template mitigate this risk?
3.  **F-string Limitations:** List the three primary benefits that `PromptTemplate` provides that a simple Python F-string implementation of a dynamic prompt would lack.
4.  **Context Management:** Explain why simply appending user input strings to a list for chat history is insufficient for maintaining conversational context, and how LangChain's message classes solve this inherent ambiguity.

### Coding Practice Questions

#### Task 1: Building a Dynamic Recipe Generator (PromptTemplate)

You need to build a tool that summarizes complex recipes from a source URL. The summary must always emphasize flavor profile, ingredient sourcing, and necessary cooking skills. The user provides only the `recipe_name`, `target_audience`, and `length`.

**Required Steps:**
a. Create a detailed dynamic template using `PromptTemplate`. Ensure the template text contains fixed instructions about flavor, sourcing, and skills.
b. Define the `input_variables` correctly.
c. Demonstrate how to invoke the template by providing values for the three dynamic inputs.
d. *Challenge:* Modify the `PromptTemplate` definition to include a validation parameter that ensures the user provides all necessary inputs.

#### Task 2: Implementing a Role-Specific Chatbot (LangChain Messages)

Create a context-aware chatbot that acts specifically as a "Sarcastic History Professor."

**Required Steps:**
a. Initialize the `chat_history` list with a `SystemMessage` defining the specific sarcastic persona.
b. Implement a `while True` loop that takes user input.
c. Within the loop, correctly convert the user input to a `HumanMessage` and append it to the history.
d. *Conceptual:* If you were to invoke the model (assume `model.invoke()` works), what type of message would the resulting LLM response need to be converted into before being saved back to the `chat_history`?

#### Task 3: Creating a Dynamic Chain Template (`ChatPromptTemplate`)

You are building a dynamic tool for teaching coding concepts. The user selects the `language` (Python, Java, etc.) and the `concept` (Loops, Recursion, etc.).

**Required Steps:**
a. Use `ChatPromptTemplate` to define the system role and the human query dynamically.
    *   System Message: "You are an expert tutor in the {language} language."
    *   Human Message: "Provide a simple, runnable code example demonstrating {concept}."
b. Demonstrate the recommended tuple syntax for defining messages within `ChatPromptTemplate`.
c. Invoke the template for Python and Recursion to generate the final list of messages.

#### Task 4: History Management via `MessagesPlaceholder`

Simulate a scenario where a user returns to a conversation after a break.

**Required Steps:**
a. Define a sample list of two or three messages representing the `prior_session_history` (ensure this list includes both `HumanMessage` and `AIMessage`).
b. Create a `ChatPromptTemplate` that includes a fixed `SystemMessage`, a `MessagesPlaceholder` for the history, and a final human query placeholder (`{new_question}`).
c. Invoke the template, passing the `prior_session_history` into the `MessagesPlaceholder` variable, and provide a new question (e.g., "Based on our last discussion, what should I do next?").

***

> **Analogy for Prompt Templates:**
>
> Think of dynamic prompt templates as using a complex **Mad Libs** form rather than writing a story from scratch. The structure (the story itself, or the LLM's operating instructions) is fixed and controlled by the programmer, ensuring high quality (guaranteeing descriptive adjectives, specific verbs, etc.). The user only provides the handful of words (input variables) needed to fill the blanks, guaranteeing that the final output follows the consistent, desired format.